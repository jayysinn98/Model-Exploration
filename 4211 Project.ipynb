{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a27f187",
   "metadata": {},
   "source": [
    "# Setting up the environment and importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce4d52da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91c40343",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xy = pd.read_csv(\"DSA4211/train-xy.csv\")\n",
    "test = pd.read_csv(\"DSA4211/test-x.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ca93f9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>...</th>\n",
       "      <th>X91</th>\n",
       "      <th>X92</th>\n",
       "      <th>X93</th>\n",
       "      <th>X94</th>\n",
       "      <th>X95</th>\n",
       "      <th>X96</th>\n",
       "      <th>X97</th>\n",
       "      <th>X98</th>\n",
       "      <th>X99</th>\n",
       "      <th>X100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.836799</td>\n",
       "      <td>-0.358848</td>\n",
       "      <td>0.229182</td>\n",
       "      <td>0.699971</td>\n",
       "      <td>1.336190</td>\n",
       "      <td>0.984788</td>\n",
       "      <td>-0.425239</td>\n",
       "      <td>1.310004</td>\n",
       "      <td>0.215709</td>\n",
       "      <td>-1.389411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.281741</td>\n",
       "      <td>-0.922837</td>\n",
       "      <td>1.053928</td>\n",
       "      <td>-1.051535</td>\n",
       "      <td>-0.056903</td>\n",
       "      <td>0.083066</td>\n",
       "      <td>0.192312</td>\n",
       "      <td>-0.039506</td>\n",
       "      <td>-0.806675</td>\n",
       "      <td>0.047861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-6.691350</td>\n",
       "      <td>1.081602</td>\n",
       "      <td>-0.455832</td>\n",
       "      <td>0.492078</td>\n",
       "      <td>1.408173</td>\n",
       "      <td>-0.197963</td>\n",
       "      <td>1.617531</td>\n",
       "      <td>-0.950866</td>\n",
       "      <td>-1.250921</td>\n",
       "      <td>-0.303370</td>\n",
       "      <td>...</td>\n",
       "      <td>2.570875</td>\n",
       "      <td>-1.028607</td>\n",
       "      <td>0.679621</td>\n",
       "      <td>-1.361717</td>\n",
       "      <td>0.144300</td>\n",
       "      <td>1.453990</td>\n",
       "      <td>-0.298901</td>\n",
       "      <td>-1.717027</td>\n",
       "      <td>-0.518633</td>\n",
       "      <td>-0.593781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.211965</td>\n",
       "      <td>-0.959496</td>\n",
       "      <td>-0.430918</td>\n",
       "      <td>0.191239</td>\n",
       "      <td>-0.910758</td>\n",
       "      <td>-0.699870</td>\n",
       "      <td>-1.683374</td>\n",
       "      <td>0.719717</td>\n",
       "      <td>0.534322</td>\n",
       "      <td>0.669822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055120</td>\n",
       "      <td>-0.344222</td>\n",
       "      <td>0.092876</td>\n",
       "      <td>-0.513097</td>\n",
       "      <td>1.317656</td>\n",
       "      <td>0.138951</td>\n",
       "      <td>-1.900100</td>\n",
       "      <td>1.496698</td>\n",
       "      <td>-0.108403</td>\n",
       "      <td>0.823793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.404807</td>\n",
       "      <td>0.611808</td>\n",
       "      <td>0.186991</td>\n",
       "      <td>0.490763</td>\n",
       "      <td>0.488140</td>\n",
       "      <td>-0.847026</td>\n",
       "      <td>1.205372</td>\n",
       "      <td>1.462078</td>\n",
       "      <td>-1.410871</td>\n",
       "      <td>-1.152401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.036887</td>\n",
       "      <td>0.573940</td>\n",
       "      <td>1.462439</td>\n",
       "      <td>-2.483321</td>\n",
       "      <td>-0.173088</td>\n",
       "      <td>0.900508</td>\n",
       "      <td>0.441478</td>\n",
       "      <td>0.461693</td>\n",
       "      <td>0.791968</td>\n",
       "      <td>-0.225677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-8.606178</td>\n",
       "      <td>0.716633</td>\n",
       "      <td>-1.377330</td>\n",
       "      <td>-2.589308</td>\n",
       "      <td>0.444174</td>\n",
       "      <td>2.859679</td>\n",
       "      <td>-0.476884</td>\n",
       "      <td>0.558184</td>\n",
       "      <td>-0.476010</td>\n",
       "      <td>-1.181636</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.734769</td>\n",
       "      <td>-0.556033</td>\n",
       "      <td>-1.528430</td>\n",
       "      <td>0.126828</td>\n",
       "      <td>2.164732</td>\n",
       "      <td>0.209919</td>\n",
       "      <td>-0.324924</td>\n",
       "      <td>-0.864985</td>\n",
       "      <td>-0.494360</td>\n",
       "      <td>-1.019706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-7.673689</td>\n",
       "      <td>0.223979</td>\n",
       "      <td>-2.227140</td>\n",
       "      <td>0.629109</td>\n",
       "      <td>0.371306</td>\n",
       "      <td>-0.623456</td>\n",
       "      <td>0.570507</td>\n",
       "      <td>-1.641528</td>\n",
       "      <td>0.345308</td>\n",
       "      <td>-1.138447</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.086697</td>\n",
       "      <td>-0.093600</td>\n",
       "      <td>-0.285808</td>\n",
       "      <td>0.486591</td>\n",
       "      <td>1.565281</td>\n",
       "      <td>-0.500644</td>\n",
       "      <td>-0.758327</td>\n",
       "      <td>0.379794</td>\n",
       "      <td>0.508518</td>\n",
       "      <td>0.558814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-13.082628</td>\n",
       "      <td>1.809326</td>\n",
       "      <td>1.459093</td>\n",
       "      <td>0.158428</td>\n",
       "      <td>0.720212</td>\n",
       "      <td>-2.549219</td>\n",
       "      <td>0.050776</td>\n",
       "      <td>-0.569455</td>\n",
       "      <td>0.646585</td>\n",
       "      <td>-0.623185</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.193381</td>\n",
       "      <td>-1.094384</td>\n",
       "      <td>0.141609</td>\n",
       "      <td>0.566846</td>\n",
       "      <td>0.742308</td>\n",
       "      <td>0.088769</td>\n",
       "      <td>-0.773133</td>\n",
       "      <td>-0.219185</td>\n",
       "      <td>-1.311174</td>\n",
       "      <td>1.753055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-4.411914</td>\n",
       "      <td>1.386181</td>\n",
       "      <td>-0.346397</td>\n",
       "      <td>1.236157</td>\n",
       "      <td>-0.324641</td>\n",
       "      <td>1.194937</td>\n",
       "      <td>-1.256203</td>\n",
       "      <td>-1.006312</td>\n",
       "      <td>0.356330</td>\n",
       "      <td>0.608835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.207263</td>\n",
       "      <td>-0.422256</td>\n",
       "      <td>0.331662</td>\n",
       "      <td>0.288487</td>\n",
       "      <td>1.912556</td>\n",
       "      <td>-0.507740</td>\n",
       "      <td>0.500480</td>\n",
       "      <td>-0.943728</td>\n",
       "      <td>0.651914</td>\n",
       "      <td>1.540073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2.900659</td>\n",
       "      <td>0.811981</td>\n",
       "      <td>1.859064</td>\n",
       "      <td>-0.251681</td>\n",
       "      <td>-0.868155</td>\n",
       "      <td>-1.598939</td>\n",
       "      <td>0.586293</td>\n",
       "      <td>-2.088541</td>\n",
       "      <td>-0.519406</td>\n",
       "      <td>-0.495364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444277</td>\n",
       "      <td>-0.495758</td>\n",
       "      <td>-1.964753</td>\n",
       "      <td>0.734672</td>\n",
       "      <td>-0.146166</td>\n",
       "      <td>-0.052333</td>\n",
       "      <td>0.587487</td>\n",
       "      <td>-0.343477</td>\n",
       "      <td>-0.213943</td>\n",
       "      <td>0.182537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>6.899348</td>\n",
       "      <td>-0.104767</td>\n",
       "      <td>1.123872</td>\n",
       "      <td>1.093857</td>\n",
       "      <td>0.246709</td>\n",
       "      <td>-0.323426</td>\n",
       "      <td>1.060075</td>\n",
       "      <td>1.165877</td>\n",
       "      <td>0.306508</td>\n",
       "      <td>-0.678595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165224</td>\n",
       "      <td>-0.949438</td>\n",
       "      <td>0.338731</td>\n",
       "      <td>-0.606249</td>\n",
       "      <td>2.410981</td>\n",
       "      <td>0.621478</td>\n",
       "      <td>0.868206</td>\n",
       "      <td>-0.441869</td>\n",
       "      <td>-0.002389</td>\n",
       "      <td>0.756689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 101 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Y        X1        X2        X3        X4        X5        X6  \\\n",
       "0    -2.836799 -0.358848  0.229182  0.699971  1.336190  0.984788 -0.425239   \n",
       "1    -6.691350  1.081602 -0.455832  0.492078  1.408173 -0.197963  1.617531   \n",
       "2     2.211965 -0.959496 -0.430918  0.191239 -0.910758 -0.699870 -1.683374   \n",
       "3    -3.404807  0.611808  0.186991  0.490763  0.488140 -0.847026  1.205372   \n",
       "4    -8.606178  0.716633 -1.377330 -2.589308  0.444174  2.859679 -0.476884   \n",
       "..         ...       ...       ...       ...       ...       ...       ...   \n",
       "995  -7.673689  0.223979 -2.227140  0.629109  0.371306 -0.623456  0.570507   \n",
       "996 -13.082628  1.809326  1.459093  0.158428  0.720212 -2.549219  0.050776   \n",
       "997  -4.411914  1.386181 -0.346397  1.236157 -0.324641  1.194937 -1.256203   \n",
       "998   2.900659  0.811981  1.859064 -0.251681 -0.868155 -1.598939  0.586293   \n",
       "999   6.899348 -0.104767  1.123872  1.093857  0.246709 -0.323426  1.060075   \n",
       "\n",
       "           X7        X8        X9  ...       X91       X92       X93  \\\n",
       "0    1.310004  0.215709 -1.389411  ...  0.281741 -0.922837  1.053928   \n",
       "1   -0.950866 -1.250921 -0.303370  ...  2.570875 -1.028607  0.679621   \n",
       "2    0.719717  0.534322  0.669822  ...  0.055120 -0.344222  0.092876   \n",
       "3    1.462078 -1.410871 -1.152401  ... -0.036887  0.573940  1.462439   \n",
       "4    0.558184 -0.476010 -1.181636  ... -1.734769 -0.556033 -1.528430   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995 -1.641528  0.345308 -1.138447  ... -1.086697 -0.093600 -0.285808   \n",
       "996 -0.569455  0.646585 -0.623185  ... -1.193381 -1.094384  0.141609   \n",
       "997 -1.006312  0.356330  0.608835  ...  0.207263 -0.422256  0.331662   \n",
       "998 -2.088541 -0.519406 -0.495364  ...  0.444277 -0.495758 -1.964753   \n",
       "999  1.165877  0.306508 -0.678595  ...  0.165224 -0.949438  0.338731   \n",
       "\n",
       "          X94       X95       X96       X97       X98       X99      X100  \n",
       "0   -1.051535 -0.056903  0.083066  0.192312 -0.039506 -0.806675  0.047861  \n",
       "1   -1.361717  0.144300  1.453990 -0.298901 -1.717027 -0.518633 -0.593781  \n",
       "2   -0.513097  1.317656  0.138951 -1.900100  1.496698 -0.108403  0.823793  \n",
       "3   -2.483321 -0.173088  0.900508  0.441478  0.461693  0.791968 -0.225677  \n",
       "4    0.126828  2.164732  0.209919 -0.324924 -0.864985 -0.494360 -1.019706  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "995  0.486591  1.565281 -0.500644 -0.758327  0.379794  0.508518  0.558814  \n",
       "996  0.566846  0.742308  0.088769 -0.773133 -0.219185 -1.311174  1.753055  \n",
       "997  0.288487  1.912556 -0.507740  0.500480 -0.943728  0.651914  1.540073  \n",
       "998  0.734672 -0.146166 -0.052333  0.587487 -0.343477 -0.213943  0.182537  \n",
       "999 -0.606249  2.410981  0.621478  0.868206 -0.441869 -0.002389  0.756689  \n",
       "\n",
       "[1000 rows x 101 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "99964555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X91</th>\n",
       "      <th>X92</th>\n",
       "      <th>X93</th>\n",
       "      <th>X94</th>\n",
       "      <th>X95</th>\n",
       "      <th>X96</th>\n",
       "      <th>X97</th>\n",
       "      <th>X98</th>\n",
       "      <th>X99</th>\n",
       "      <th>X100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.244991</td>\n",
       "      <td>0.892978</td>\n",
       "      <td>0.572407</td>\n",
       "      <td>0.132279</td>\n",
       "      <td>0.846642</td>\n",
       "      <td>0.096176</td>\n",
       "      <td>0.892746</td>\n",
       "      <td>0.924040</td>\n",
       "      <td>-0.434594</td>\n",
       "      <td>-1.144520</td>\n",
       "      <td>...</td>\n",
       "      <td>0.906264</td>\n",
       "      <td>1.901898</td>\n",
       "      <td>0.451177</td>\n",
       "      <td>0.292590</td>\n",
       "      <td>0.629340</td>\n",
       "      <td>-0.222918</td>\n",
       "      <td>0.246944</td>\n",
       "      <td>0.344474</td>\n",
       "      <td>0.135516</td>\n",
       "      <td>0.738561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.616918</td>\n",
       "      <td>-1.113591</td>\n",
       "      <td>-1.202111</td>\n",
       "      <td>0.427342</td>\n",
       "      <td>1.022719</td>\n",
       "      <td>-1.012529</td>\n",
       "      <td>-1.518107</td>\n",
       "      <td>2.199862</td>\n",
       "      <td>0.327463</td>\n",
       "      <td>-1.338159</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.578023</td>\n",
       "      <td>0.824843</td>\n",
       "      <td>-0.478459</td>\n",
       "      <td>1.914525</td>\n",
       "      <td>-0.487439</td>\n",
       "      <td>1.386022</td>\n",
       "      <td>-0.212164</td>\n",
       "      <td>0.221503</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>0.511010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-2.261368</td>\n",
       "      <td>0.690216</td>\n",
       "      <td>-1.228782</td>\n",
       "      <td>-0.279688</td>\n",
       "      <td>0.832744</td>\n",
       "      <td>0.103335</td>\n",
       "      <td>-0.074730</td>\n",
       "      <td>0.616347</td>\n",
       "      <td>-0.085210</td>\n",
       "      <td>0.176623</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.227196</td>\n",
       "      <td>1.933549</td>\n",
       "      <td>0.424807</td>\n",
       "      <td>-0.351631</td>\n",
       "      <td>-1.532469</td>\n",
       "      <td>-2.051089</td>\n",
       "      <td>1.021578</td>\n",
       "      <td>1.738350</td>\n",
       "      <td>-0.986476</td>\n",
       "      <td>-0.678784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.370109</td>\n",
       "      <td>-0.121966</td>\n",
       "      <td>0.911579</td>\n",
       "      <td>-1.684504</td>\n",
       "      <td>0.243944</td>\n",
       "      <td>0.004523</td>\n",
       "      <td>-0.690897</td>\n",
       "      <td>-0.584691</td>\n",
       "      <td>2.780507</td>\n",
       "      <td>1.696416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.197386</td>\n",
       "      <td>1.596440</td>\n",
       "      <td>-0.143426</td>\n",
       "      <td>1.838676</td>\n",
       "      <td>-0.090792</td>\n",
       "      <td>0.501838</td>\n",
       "      <td>0.720952</td>\n",
       "      <td>-0.822403</td>\n",
       "      <td>-0.787266</td>\n",
       "      <td>-0.553230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.983610</td>\n",
       "      <td>0.037572</td>\n",
       "      <td>0.573795</td>\n",
       "      <td>-0.078239</td>\n",
       "      <td>1.561335</td>\n",
       "      <td>-0.276647</td>\n",
       "      <td>-0.741170</td>\n",
       "      <td>0.970027</td>\n",
       "      <td>0.124756</td>\n",
       "      <td>0.855003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.380206</td>\n",
       "      <td>-1.856744</td>\n",
       "      <td>0.403040</td>\n",
       "      <td>0.251322</td>\n",
       "      <td>-0.587490</td>\n",
       "      <td>-1.399546</td>\n",
       "      <td>0.589233</td>\n",
       "      <td>-0.675056</td>\n",
       "      <td>-0.308820</td>\n",
       "      <td>0.742988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>-0.819463</td>\n",
       "      <td>-0.132225</td>\n",
       "      <td>1.987259</td>\n",
       "      <td>1.482543</td>\n",
       "      <td>-0.153700</td>\n",
       "      <td>-1.182747</td>\n",
       "      <td>1.134379</td>\n",
       "      <td>1.433691</td>\n",
       "      <td>-1.750373</td>\n",
       "      <td>1.535513</td>\n",
       "      <td>...</td>\n",
       "      <td>2.253787</td>\n",
       "      <td>0.127843</td>\n",
       "      <td>1.099243</td>\n",
       "      <td>-0.027314</td>\n",
       "      <td>0.508060</td>\n",
       "      <td>-0.291674</td>\n",
       "      <td>0.142223</td>\n",
       "      <td>0.334328</td>\n",
       "      <td>0.195834</td>\n",
       "      <td>0.886085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-1.441815</td>\n",
       "      <td>-0.520422</td>\n",
       "      <td>-1.338284</td>\n",
       "      <td>-0.720087</td>\n",
       "      <td>1.778857</td>\n",
       "      <td>0.042734</td>\n",
       "      <td>0.269349</td>\n",
       "      <td>-1.718217</td>\n",
       "      <td>1.115974</td>\n",
       "      <td>-0.617458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147480</td>\n",
       "      <td>-1.023812</td>\n",
       "      <td>-0.233869</td>\n",
       "      <td>-0.013129</td>\n",
       "      <td>0.047350</td>\n",
       "      <td>-0.261349</td>\n",
       "      <td>0.963827</td>\n",
       "      <td>-1.238524</td>\n",
       "      <td>-1.672828</td>\n",
       "      <td>-0.074160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>0.079223</td>\n",
       "      <td>0.988820</td>\n",
       "      <td>0.407241</td>\n",
       "      <td>-0.142056</td>\n",
       "      <td>-0.633980</td>\n",
       "      <td>0.800763</td>\n",
       "      <td>-0.535756</td>\n",
       "      <td>1.134397</td>\n",
       "      <td>-1.387698</td>\n",
       "      <td>0.321071</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.881513</td>\n",
       "      <td>0.323094</td>\n",
       "      <td>-0.653722</td>\n",
       "      <td>0.269482</td>\n",
       "      <td>-0.038042</td>\n",
       "      <td>-0.617090</td>\n",
       "      <td>-0.236006</td>\n",
       "      <td>0.230748</td>\n",
       "      <td>-0.501167</td>\n",
       "      <td>-0.072894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>-0.768939</td>\n",
       "      <td>-0.921212</td>\n",
       "      <td>-0.260985</td>\n",
       "      <td>-0.102339</td>\n",
       "      <td>1.568072</td>\n",
       "      <td>0.084913</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>0.481204</td>\n",
       "      <td>0.823396</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475284</td>\n",
       "      <td>-0.556861</td>\n",
       "      <td>1.729765</td>\n",
       "      <td>0.096702</td>\n",
       "      <td>1.169640</td>\n",
       "      <td>0.388464</td>\n",
       "      <td>-1.440724</td>\n",
       "      <td>-2.590144</td>\n",
       "      <td>1.665079</td>\n",
       "      <td>-0.352464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>0.401373</td>\n",
       "      <td>-0.834319</td>\n",
       "      <td>-0.590497</td>\n",
       "      <td>0.030925</td>\n",
       "      <td>-0.789554</td>\n",
       "      <td>-0.905707</td>\n",
       "      <td>0.130780</td>\n",
       "      <td>-0.945585</td>\n",
       "      <td>1.302955</td>\n",
       "      <td>-2.144070</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194804</td>\n",
       "      <td>1.357549</td>\n",
       "      <td>-0.758704</td>\n",
       "      <td>0.074982</td>\n",
       "      <td>-1.068588</td>\n",
       "      <td>-0.918793</td>\n",
       "      <td>-0.572129</td>\n",
       "      <td>0.345093</td>\n",
       "      <td>-1.409649</td>\n",
       "      <td>0.043278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0    -0.244991  0.892978  0.572407  0.132279  0.846642  0.096176  0.892746   \n",
       "1    -1.616918 -1.113591 -1.202111  0.427342  1.022719 -1.012529 -1.518107   \n",
       "2    -2.261368  0.690216 -1.228782 -0.279688  0.832744  0.103335 -0.074730   \n",
       "3     1.370109 -0.121966  0.911579 -1.684504  0.243944  0.004523 -0.690897   \n",
       "4     1.983610  0.037572  0.573795 -0.078239  1.561335 -0.276647 -0.741170   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "9995 -0.819463 -0.132225  1.987259  1.482543 -0.153700 -1.182747  1.134379   \n",
       "9996 -1.441815 -0.520422 -1.338284 -0.720087  1.778857  0.042734  0.269349   \n",
       "9997  0.079223  0.988820  0.407241 -0.142056 -0.633980  0.800763 -0.535756   \n",
       "9998 -0.768939 -0.921212 -0.260985 -0.102339  1.568072  0.084913  0.980283   \n",
       "9999  0.401373 -0.834319 -0.590497  0.030925 -0.789554 -0.905707  0.130780   \n",
       "\n",
       "            X8        X9       X10  ...       X91       X92       X93  \\\n",
       "0     0.924040 -0.434594 -1.144520  ...  0.906264  1.901898  0.451177   \n",
       "1     2.199862  0.327463 -1.338159  ... -1.578023  0.824843 -0.478459   \n",
       "2     0.616347 -0.085210  0.176623  ... -0.227196  1.933549  0.424807   \n",
       "3    -0.584691  2.780507  1.696416  ...  0.197386  1.596440 -0.143426   \n",
       "4     0.970027  0.124756  0.855003  ...  1.380206 -1.856744  0.403040   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  1.433691 -1.750373  1.535513  ...  2.253787  0.127843  1.099243   \n",
       "9996 -1.718217  1.115974 -0.617458  ... -0.147480 -1.023812 -0.233869   \n",
       "9997  1.134397 -1.387698  0.321071  ... -0.881513  0.323094 -0.653722   \n",
       "9998  0.481204  0.823396  0.033000  ... -0.475284 -0.556861  1.729765   \n",
       "9999 -0.945585  1.302955 -2.144070  ... -0.194804  1.357549 -0.758704   \n",
       "\n",
       "           X94       X95       X96       X97       X98       X99      X100  \n",
       "0     0.292590  0.629340 -0.222918  0.246944  0.344474  0.135516  0.738561  \n",
       "1     1.914525 -0.487439  1.386022 -0.212164  0.221503  0.036357  0.511010  \n",
       "2    -0.351631 -1.532469 -2.051089  1.021578  1.738350 -0.986476 -0.678784  \n",
       "3     1.838676 -0.090792  0.501838  0.720952 -0.822403 -0.787266 -0.553230  \n",
       "4     0.251322 -0.587490 -1.399546  0.589233 -0.675056 -0.308820  0.742988  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995 -0.027314  0.508060 -0.291674  0.142223  0.334328  0.195834  0.886085  \n",
       "9996 -0.013129  0.047350 -0.261349  0.963827 -1.238524 -1.672828 -0.074160  \n",
       "9997  0.269482 -0.038042 -0.617090 -0.236006  0.230748 -0.501167 -0.072894  \n",
       "9998  0.096702  1.169640  0.388464 -1.440724 -2.590144  1.665079 -0.352464  \n",
       "9999  0.074982 -1.068588 -0.918793 -0.572129  0.345093 -1.409649  0.043278  \n",
       "\n",
       "[10000 rows x 100 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b8e195",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb31700",
   "metadata": {},
   "source": [
    "X30 and X40 are highly correlated, so I will drop the latter.\n",
    "\n",
    "X55 contains mostly NAs and shall be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfdb9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xy1 = train_xy.drop(['X40','X55'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fac09",
   "metadata": {},
   "source": [
    "# Splitting the data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d888f6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_xy1.iloc[:,1:100]\n",
    "y = train_xy.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6a9a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efb33f",
   "metadata": {},
   "source": [
    "# Building the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45cdb57",
   "metadata": {},
   "source": [
    "Each of the 98 variables shall be an input node.\n",
    "\n",
    "Based on previous models (not in this notebook), data is not linearly seperable, hence a hidden layer will be added.\n",
    "\n",
    "This hidden layer will contain 49 neurons, obtained by taking the mean of the number of input and output neurons.\n",
    "\n",
    "Since this is a regression problem, ReLU activation function and Mean Squared Error loss is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b44bbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(): \n",
    "    model = Sequential()\n",
    "    model.add(Dense(98, input_dim=98, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(49, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445f39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=build_model, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "training_prediction = estimator.predict(X_train)\n",
    "training_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb9a49",
   "metadata": {},
   "source": [
    "# Checking initial model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b6c240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rss = sum((training_prediction-y_train)**2)\n",
    "tss = sum((y_train - (sum(y_train)/len(y_train)))**2)\n",
    "rsq = 1 - (rss/tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d10f6222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9965658536562861"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d442827",
   "metadata": {},
   "source": [
    "Training R-squared of 0.997 looks very good. How about on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8e4c281",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_prediction = estimator.predict(X_test)\n",
    "rss = sum((validation_prediction-y_test)**2)\n",
    "tss = sum((y_test - (sum(y_test)/len(y_test)))**2)\n",
    "rsq = 1 - (rss/tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d28177dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4680385401096595"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rsq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42ba0fc",
   "metadata": {},
   "source": [
    "Major drop in R-squared, indicating serious overfitting. \n",
    "\n",
    "I will edit the model by removing the hidden layer, which may be a source of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463c4e5c",
   "metadata": {},
   "source": [
    "# Adjusted Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d9fd23fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(): \n",
    "    model = Sequential()\n",
    "    model.add(Dense(98, input_dim=98, kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dense(49, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ef5d23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=build_model2, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "training_prediction = estimator.predict(X_train)\n",
    "validation_prediction = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "807da81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training R-squared is : 0.9962254827554976\n"
     ]
    }
   ],
   "source": [
    "rss = sum((training_prediction-y_train)**2)\n",
    "tss = sum((y_train - (sum(y_train)/len(y_train)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The training R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "65799eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation R-squared is : 0.5079300946329504\n"
     ]
    }
   ],
   "source": [
    "rss = sum((validation_prediction-y_test)**2)\n",
    "tss = sum((y_test - (sum(y_test)/len(y_test)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The validation R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0ba077",
   "metadata": {},
   "source": [
    "# Regularized and Adjusted Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58286141",
   "metadata": {},
   "source": [
    "Let's see if adding drop out will help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f5e454c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model3(): \n",
    "    model = Sequential()\n",
    "    model.add(Dropout(0.2, input_shape=(98,)))\n",
    "    model.add(Dense(98, input_dim=98, kernel_initializer='normal', activation='relu'))\n",
    "    #model.add(Dense(49, kernel_initializer='normal', activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer='normal'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4f0b74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=build_model2, epochs=100, batch_size=5, verbose=0)\n",
    "estimator.fit(X_train, y_train)\n",
    "training_prediction = estimator.predict(X_train)\n",
    "validation_prediction = estimator.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c583f839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training R-squared is : 0.9947759623057474\n"
     ]
    }
   ],
   "source": [
    "rss = sum((training_prediction-y_train)**2)\n",
    "tss = sum((y_train - (sum(y_train)/len(y_train)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The training R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "caaeeb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation R-squared is : 0.4858238887488415\n"
     ]
    }
   ],
   "source": [
    "rss = sum((validation_prediction-y_test)**2)\n",
    "tss = sum((y_test - (sum(y_test)/len(y_test)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The validation R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b56ed5",
   "metadata": {},
   "source": [
    "Validation R-squared still very low for both. Perhaps a simpler model should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4ed1c2",
   "metadata": {},
   "source": [
    "# Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ec9fb2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de326a74",
   "metadata": {},
   "source": [
    "10-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2c140423",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LassoCV(cv=10, normalize=False).fit(X_train, y_train)\n",
    "training_prediction = reg.predict(X_train)\n",
    "validation_prediction = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c63f2321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training R-squared is : 0.6315625160196948\n"
     ]
    }
   ],
   "source": [
    "rss = sum((training_prediction-y_train)**2)\n",
    "tss = sum((y_train - (sum(y_train)/len(y_train)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The training R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c4b2136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation R-squared is : 0.5761903999867041\n"
     ]
    }
   ],
   "source": [
    "rss = sum((validation_prediction-y_test)**2)\n",
    "tss = sum((y_test - (sum(y_test)/len(y_test)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The validation R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d2f5a",
   "metadata": {},
   "source": [
    "Even though training R-squared here is much lower, the Lasso model performs better on the validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6423c8fc",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d4406410",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c549987",
   "metadata": {},
   "source": [
    "Start with a max depth of 20, which is arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "951834b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(max_depth=20, random_state=0)\n",
    "regr.fit(X_train, y_train)\n",
    "training_prediction = regr.predict(X_train)\n",
    "validation_prediction = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "976a2dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training R-squared is : 0.9326960383177906\n"
     ]
    }
   ],
   "source": [
    "rss = sum((training_prediction-y_train)**2)\n",
    "tss = sum((y_train - (sum(y_train)/len(y_train)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The training R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "54793f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation R-squared is : 0.5081193374782587\n"
     ]
    }
   ],
   "source": [
    "rss = sum((validation_prediction-y_test)**2)\n",
    "tss = sum((y_test - (sum(y_test)/len(y_test)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The validation R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc6b9cd",
   "metadata": {},
   "source": [
    "Performance is similar to that of Neural Network's. Let's check which value of max depth returns the lowest validation R-squared value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "21045492",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_array = []\n",
    "validation_array = []\n",
    "for i in range(1,20):\n",
    "    regr = RandomForestRegressor(max_depth=i, random_state=0)\n",
    "    regr.fit(X_train, y_train)\n",
    "    training_prediction = regr.predict(X_train)\n",
    "    validation_prediction = regr.predict(X_test)\n",
    "    \n",
    "    rss = sum((training_prediction-y_train)**2)\n",
    "    tss = sum((y_train - (sum(y_train)/len(y_train)))**2)\n",
    "    rsq = 1 - (rss/tss)\n",
    "    training_array.append(rsq)\n",
    "    \n",
    "    rss = sum((validation_prediction-y_test)**2)\n",
    "    tss = sum((y_test - (sum(y_test)/len(y_test)))**2)\n",
    "    rsq = 1 - (rss/tss)\n",
    "    validation_array.append(rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a466975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "41644f5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1,20), validation_array, label = 'Validation R-squared')\n",
    "plt.plot(range(1,20), training_array, label = 'Training R-squared')\n",
    "plt.title('Training and Validation R-squared against Max Depth')\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('R-squared')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7778d58",
   "metadata": {},
   "source": [
    "Once the max depth hits 10, validation R-squared is pretty much constant. Thus I will check what's the exact R-squared value for a Random Forest with max depth of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "83d8a173",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr2 = RandomForestRegressor(max_depth=10, random_state=0)\n",
    "regr2.fit(X_train, y_train)\n",
    "training_prediction = regr2.predict(X_train)\n",
    "validation_prediction = regr2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2a2715b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training R-squared is : 0.9191561864229907\n"
     ]
    }
   ],
   "source": [
    "rss = sum((training_prediction-y_train)**2)\n",
    "tss = sum((y_train - (sum(y_train)/len(y_train)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The training R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0063efd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The validation R-squared is : 0.5071896303474293\n"
     ]
    }
   ],
   "source": [
    "rss = sum((validation_prediction-y_test)**2)\n",
    "tss = sum((y_test - (sum(y_test)/len(y_test)))**2)\n",
    "rsq = 1 - (rss/tss)\n",
    "print('The validation R-squared is :', rsq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a358e",
   "metadata": {},
   "source": [
    "Validation R-squared is almost the same as that of max depth 20, but training R-squared has reduced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a478a0",
   "metadata": {},
   "source": [
    "# Running the chosen model on the test set for my final predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104ab2e",
   "metadata": {},
   "source": [
    "Since the model will be assessed using a test set with no response, choosing the model with the highest validation, not training, R-squared should be the way to go.\n",
    "\n",
    "Based on the 3 above models, Lasso had the highest validation R-squared. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e5e4d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds = pd.DataFrame(reg.predict(test.drop(['X40','X55'],axis = 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3e07e2bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.218286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.075417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.876225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.984139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8.148008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>6.639682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>-2.066169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>-0.011487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>5.032062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>-6.029369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Y\n",
       "0     -2.218286\n",
       "1      0.075417\n",
       "2      1.876225\n",
       "3     10.984139\n",
       "4      8.148008\n",
       "...         ...\n",
       "9995   6.639682\n",
       "9996  -2.066169\n",
       "9997  -0.011487\n",
       "9998   5.032062\n",
       "9999  -6.029369\n",
       "\n",
       "[10000 rows x 1 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_preds.columns = ['Y']\n",
    "final_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "90d2bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/jasinchow/Downloads/A0204765A.csv'\n",
    "final_preds.to_csv(path, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
